{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/Aishu/dl-model-extraction'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "idQmKzBgZqug"
   },
   "outputs": [],
   "source": [
    "from attacker.query import *\n",
    "from attacker.utils import *\n",
    "from victim.__init__ import *\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9SehmoStA7J"
   },
   "source": [
    "# Query victim model & Train attacker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get dataset in dataloader\n",
    "trainloader, testloader, outputs = get_dataloader(config[\"victim\"][\"data\"])\n",
    "# initialize attacker model\n",
    "model = get_model(config[\"attacker\"], outputs)\n",
    "# train attacker model\n",
    "_ = training(model, trainloader, testloader)\n",
    "\n",
    "save_visualize(None, _,'title' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lMNfr2wZwGMF",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Loading queried cifar_10 dataset with resnet50 victim\n",
      "    - input:10000 queried:10000\n",
      "Sample using None with query size 10000\n",
      "    - input:10000 sampled:10000 data augmentation:False\n",
      "Loading queried cifar_10 dataset with resnet50 victim\n",
      "    - input:50000 queried:50000\n",
      "Sample using coreset with query size 20000\n",
      "    - input:50000 sampled:20000 data augmentation:True\n",
      "Total epochs: 100\n",
      "epoch 1\n",
      "    - train_acc 0.41710 train_loss 1.36696\n",
      "    - test_acc  0.28590 test_loss  4.05674\n",
      "epoch 2\n",
      " 17000 ..."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28410/1888896225.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# train attacker model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mattacker_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattacker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquerytrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquerytestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# save & visualize model inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dl-model-extraction/attacker/utils.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, train_loader, test_loader)\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0mtrain_loss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mtrain_loss_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m                 \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    151\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                    \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                    maximize=group['maximize'])\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get dataset in dataloader\n",
    "trainloader, testloader, outputs = get_dataloader(config[\"victim\"][\"data\"])\n",
    "\n",
    "# query & save test data\n",
    "querytestloader = query_victim(config[\"victim\"], outputs, testloader, len(testloader.dataset), train=False)\n",
    "\n",
    "# query train data\n",
    "querytrainloader = query_victim(config[\"victim\"], outputs, trainloader, config[\"query_size\"], q_type=config[\"query_type\"])\n",
    "\n",
    "# initialize attacker model\n",
    "attacker = get_model(config[\"attacker\"], outputs)\n",
    "\n",
    "# train attacker model\n",
    "attacker_result = training(attacker, querytrainloader, querytestloader)\n",
    "\n",
    "# save & visualize model inference\n",
    "title = f'Trial-A_{config[\"attacker\"]}_{config[\"victim\"][\"model_name\"]}_{config[\"victim\"][\"data\"]}{config[\"query_type\"]}_{config[\"query_size\"]}'\n",
    "percent = save_visualize(attacker, attacker_result,title )\n",
    "results_dict={\"Victim\":config[\"victim\"][\"model_name\"],\n",
    "                \"Dataset\":config[\"victim\"][\"data\"],\n",
    "                \"QueryType\": config[\"query_type\"],\n",
    "                \"QuerySize\": config[\"query_size\"],\n",
    "                \"Queried Output\": 'Labels',\n",
    "                \"Attacker\": config[\"attacker\"],\n",
    "                \"Train Loss\": percent[0],\n",
    "                \"Train Accuracy\": percent[1],\n",
    "                \"Test Loss\": percent[2],\n",
    "                \"Test Accuracy\": percent[3]}\n",
    "print(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Case 1: Corseset vs Random ,\n",
    "    Attacker - Resnet34\n",
    "    Victim - Resnet50\n",
    "    klogits - 0\n",
    "    Datasest - Cifar10, Cifar100\n",
    "    QueryType - Random, Coreset\n",
    "    Query Size - 10k, 20k, 30k, 40k, 50k"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "parameters = {\n",
    "        \"query_size\": [10000, 20000, 30000, 40000, 50000],\n",
    "        \"query_type\": ['coreset','random'],\n",
    "        \"victim\":[{ \"data\": CIFAR_10, \"model_name\": RESNET50}],\n",
    "        \"attacker\":[RESNET34]\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "parameters = {\n",
    "        \"query_size\": [10000, 20000, 30000, 40000, 50000],\n",
    "        \"query_type\": ['coreset','random'],\n",
    "        \"victim\":[{\"data\": CIFAR_100, \"model_name\": RESNET50 }],\n",
    "        \"attacker\":[RESNET34]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune search for query size selection\n",
    "parameters = {\n",
    "        \"query_size\": [15000, 25000],\n",
    "        \"query_type\": ['coreset','random'],\n",
    "        \"victim\":[{ \"data\": CIFAR_10, \"model_name\": RESNET50}],\n",
    "        \"attacker\":[RESNET34]\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Case 2 : Victim vs Attacker\n",
    "    Dataset - Cifar10\n",
    "    klogits - 0\n",
    "    QueryType - Random\n",
    "    Query Size - 10k, 30k, 50k\n",
    "    Victim - R34, R50, VGG19\n",
    "    Attacker - R34, R50, VGG19"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Case 3 : K-Logits vs Labels\n",
    "    Dataset - Cifar10\n",
    "    klogits - 0, 3, 10\n",
    "    QueryType - Random\n",
    "    Query Size - 10k, 30k, 50k\n",
    "    Attacker - Resnet34\n",
    "    Victim - Resnet50"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Case 4 : Out of distribution Dataset\n",
    "    Dataset - ood\n",
    "    klogits - 0\n",
    "    QueryType - Random\n",
    "    Query Size - 10k, 30k, 50k\n",
    "    Attacker - Resnet34\n",
    "    Victim - Resnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'config={config}\\n\\nparameters={parameters}\\n')\n",
    "Results = []\n",
    "# Iterate through Victim Model & Dataset\n",
    "for victim in parameters[\"victim\"]:\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    # get dataset in dataloader\n",
    "    trainloader, testloader, outputs = get_dataloader(victim[\"data\"])\n",
    "\n",
    "    # query test data\n",
    "    querytestloader = query_victim(victim, outputs, testloader, len(testloader.dataset), train=False)\n",
    "\n",
    "    # Iterate through Attacker Model\n",
    "    for attacker_type in parameters[\"attacker\"]:\n",
    "        # Iterate Through Query Type\n",
    "        for querytype in parameters[\"query_type\"]:\n",
    "            # Iterate Through Query Size\n",
    "            for size in parameters[\"query_size\"]:\n",
    "                print('-----------------------------------------------------------------------------')\n",
    "                print(f'-----------------------Dataset: {victim[\"data\"]}----------------------------')\n",
    "                print(f'--------Victim: {victim[\"model_name\"]} Attacker: {attacker_type}------------')\n",
    "                print(f'---------------Query Type: {querytype} Query Size: {size}-------------------')\n",
    "\n",
    "                # query train data\n",
    "                querytrainloader = query_victim(victim, outputs, trainloader, size, q_type=querytype)\n",
    "\n",
    "                # initialize attacker model\n",
    "                attacker = get_model(attacker_type, outputs)\n",
    "\n",
    "                # train attacker model\n",
    "                attacker_result = training(attacker, querytrainloader, querytestloader)\n",
    "\n",
    "                # save & visualize model inference\n",
    "                title = f'A_{attacker_type}_{victim[\"model_name\"]}_{victim[\"data\"]}_{querytype}_{size}'\n",
    "                percent = save_visualize(attacker, attacker_result,title )\n",
    "                results_dict={\"Victim\":victim[\"model_name\"],\n",
    "                                \"Dataset\":victim[\"data\"],\n",
    "                                \"QueryType\": querytype,\n",
    "                                \"QuerySize\": size,\n",
    "                                \"Queried Output\": 'Labels',\n",
    "                                \"Attacker\": attacker_type,\n",
    "                                \"Train Loss\": percent[0],\n",
    "                                \"Train Accuracy\": percent[1],\n",
    "                                \"Test Loss\": percent[2],\n",
    "                                \"Test Accuracy\": percent[3]}\n",
    "\n",
    "                print(results_dict)\n",
    "                Results.append(results_dict)\n",
    "                print('-----------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL_Project",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
